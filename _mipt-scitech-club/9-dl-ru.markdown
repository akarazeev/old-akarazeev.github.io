---
title: MIPT DL Club \#9
lang: ru
ref: 9dl
date: 2017-11-21 03:00:01 +03:00

tags:
- MIPT DL Club
- Neural Network
- Language Modeling
- Image Synthesis
---

Очень большие нейронные сети, состоящие из слоев экспертов - _Emil Zakirov_ про ["Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"](https://arxiv.org/abs/1701.06538)

"Авторы статьи исследовали conditional computation слой под названием Mixture of Experts (MoE), разные части которого активируюся на разные примеры. За счет этого можно увеличить количество параметров в 1000 раз, сохраняя постоянным время предсказания на одном примере.

Одна из причин хороших результатов в этой статье - достаточный для такого количества параметров датасет, состоящий из 1 миллиарда предложений на разных языках. Результаты побили state of the art в задачах моделирования языков и многоязычных переводах, что не удивительно, ведь в авторах ученые из Google Translator."

{% include video id="nNZceFX2tQU" provider="youtube" %}
